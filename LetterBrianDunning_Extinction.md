Dear Brian,

I am impressed at how you take on kooky-sounding topics and give them a fair hearing. There’s a topic I’ve been looking into for the past year, and would love your sceptical perspective on: AI extinction risk

They claim that aligning artificial intelligence to human goals is extremely difficult and needs to be prioritised over making the AIs more capable. I found their claims logical, and confirmed by recent issues found in AIs. I am interested how you interpret the research and what conclusion you come to.

Now is a particularly important time to know the truth about this. At least #% of the AI research field (which is broader than the AI alignment field) believes AGI will come within ## years (linkie). The US government is considering how to regulate AI development. If AI alignment is as difficult as suggested, then we need to strongly regulate its development. Conversely if alignment is easy, then we risk losing the innovation and wealth from AI.

https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/
Letter from Ryl

Dear Brian,

Your podcast and youtube series have changed my mind on  many things, including topics I thought I was well informed on (e.g. nutrition and organic food).

Is artificial intelligence the greatest threat to humanity? Some key facts that concern me:
$$$ spent on developing AGI vs spent on researching the likely consequences of it


I understand you have thousands of ideas for sceptoid shows now. I think this one should go near the top of the list.

Kind regards, 
Ryl


