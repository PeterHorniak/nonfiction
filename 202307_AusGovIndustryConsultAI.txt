TODO: Review AI safety newsletter #12 on US discussion paper.

*Definitions*
1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer
and why?
Answer: 
     The definition of "Artificial intelligence (AI)" does not mention agentic behaviour. Agentic behaviour is a key risk of future models. Could the definition please be expanded to "AI systems are designed to operate with
varying levels of automation and agentic behaviour." 

*Potential gaps in approaches*
2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?
Answer: 
     Australia's existing regulatory approaches focus on what happens after a model is deployed. These approaches do not address the risks that exist while training a model. It is possible for model to cause harm before deployment, for example by being stolen, leaked, or by it choosing to escape.
     Another risk is that currently Australia cannot stop the training of a model that has a high probability of being deployed in future and then causing harm. One approach to mitigate this risk is to require supercomputer chips designed for AI training runs to be uniquely identifiable and trackable.
     Another risk is that currently AI-generated content cannot be identified as AI-generated by just analysing the end product. We need mandatory rules to cryptographically mark any AI-generated content with the model that generated it, the company using that model, and the purpose for which it was generated. This cryptographic mark should be impossible for downstream applications to remove. Any AI-generated content that is photorealistic and does not have such a cryptographic mark should be illegal to share on social media. 
     Another risk is the race dynamic between the leading AI labs. Their incentives are to create more powerful models and release them quickly. We need collaboration between the leading AI labs to slow development of capabilities sufficiently for development of alignment to catch up. It is unclear whether the existing regulatory approaches would consider this as antitrust behaviour.

3. Are there any further non-regulatory initiatives the Australian Government could implement to support responsible AI practices in Australia? Please describe these and their benefits or impacts.
Answer:
     Australia has limited ability to influence AI development overseas, and there is a risk of such development having negative externalities for Australia. This can be mitigated by offering permanent residency to foreign AI researchers, as they would then be work within Australian law. Due to the relatively small number of AI researchers, and Australia's high living standard, this could significantly impact AI safety research. This does not consider the economic benefit to Australia of becoming an AI hub.
     Australia could support international initiatives for safe AI development such as tracking AI-specific chips. Australia could encourage domestic AI development to use only trackable supercompute. This would enable future regulation of AI chips if that becomes necessary.

4. Do you have suggestions on coordination of AI governance across government? Please outline the goals that any coordination mechanisms could achieve and how they could influence the development and uptake of AI in Australia.
Answer:
     Agencies could use shared registers of resources for AI development. For example, a register of training datasets that identify what biases may be present. Any new models should be linked to which of those datasets were used to train it.
     Agencies could also maintain public registers of AI companies and their safety ratings. This would reward the most safety-conscious companies. The safety ratings would need to be chosen so that they accurately reflect actual safety and don't become victims of Goodhart's law.

*Responses suitable for Australia*
5. Are there any governance measures being taken or considered by other countries (including any not discussed in this paper) that are relevant, adaptable and desirable for Australia?
Answer:
     No comment.

*Target areas*
6. Should different approaches apply to public and private sector use of AI technologies? If so, how should the approaches differ?
Answer:
     Yes. The majority of leading AI research is currently done by the private sector. This is where most risk exists, and should therefore be the main focus of risk mitigation. 
     TODO: describe how approach should differ.

7. How can the Australian Government further support responsible AI practices in its own agencies?
Answer:
     Make datasets available for AI development that are already high-quality and documented. These can be made of synthetic data to avoid privacy risk. This will let AI developers spend less time on data-gathering.

8. In what circumstances are generic solutions to the risks of AI most valuable? And in what circumstances are technology-specific solutions better? Please provide some examples.
Answer:
     Generic solutions are most valuable when we want the solution to apply to technologies that have not been created yet or are not widely understood. 
     Technology-specific solutions are better when we know a particular approach to AI development is dangerous. For example, using Reinforcement Learning From Human Feedback (RLHF) relies on humans ranking outputs of AI models, and thus requires the human reviewers to understand which outputs are better or worse. This approach breaks down when quality human feedback does not scale, or when the model is intelligent enough to manipulate the feedback process.
     TODO: add example for generic solution.

9. Given the importance of transparency across the AI lifecycle, please share your thoughts on:
     a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?
     b. mandating transparency requirements across the private and public sectors, including how these requirements could be implemented.
Answer:
     For a), transparency will be most critical when describing the data used to train models, and the results of safety testing on the models.
     For b), transparency should not lead to releasing info that can speed up capabilities development across the industry. The focus should be on making safety research available across the industry.

10. Do you have suggestions for:
a. Whether any high-risk AI applications or technologies should be banned completely?
-> models of bigger size than gpt4
-> autonomous weapon use in police and border protection 
-> pentesting
-> training methods that are whackamole by design, eg RLHF
b. Criteria or requirements to identify AI applications or technologies that should be
banned, and in which contexts?
-> threshold of it becoming agentic. Need scientific consensus that a given tech will not become agentic.


11. What initiatives or government action can increase public trust in AI deployment to encourage more people to use AI?
-> require insurance for ai companies , with payouts going to victims 
-> ubi, so even if all lose jobs, can still be better off


*Implications and infrastructure*
12. How would banning high-risk activities (like social scoring or facial recognition technology in certain circumstances) impact Australia’s tech sector and our trade and exports with other countries?
-> cannot profit off selling them
-> would make devs focus on other areas that might even be more profitable because the uses wont be banned later on.
13. What changes (if any) to Australian conformity infrastructure might be required to support assurance processes to mitigate against potential AI risks?
-> todo: lkp what is conformity infra



*Risk-based approaches*
14. Do you support a risk-based approach for addressing potential AI risks? If not, is there a better approach?
-> combine with required insurances, let private insurance companies do additional diligence


15. What do you see as the main benefits or limitations of a risk-based approach? How can any limitations be overcome?
-> thresholds can be arbitrary 
-> ai devs might not know what category their stuff falls under
-> limitation is that something can be shortterm low risk and longterm high risk. What time horizons are for the assessment?
-> when there is uncertainty, does it default as higher risk?


16. Is a risk-based approach better suited to some sectors, AI applications or organisations than others based on organisation size, AI maturity and resources?
->

17. What elements should be in a risk-based approach for addressing potential AI risks? Do you support the elements presented in Attachment C?

18. How can an AI risk-based approach be incorporated into existing assessment frameworks (like privacy) or risk management processes to streamline and reduce potential duplication?

19. How might a risk-based approach apply to general purpose AI systems, such as large language models (LLMs) or multimodal foundation models (MFMs)?
-> hard to fo transparency on them due to inscrutable floating point matrices

20. Should a risk-based approach for responsible AI be a voluntary or self-regulation tool or be mandated through regulation? And should it apply to:
a. public or private organisations or both?
b. developers or deployers or both?
-> mandated, apply to all. Developers can have cruddy security and have their tech stolen and misused , and then be responsible for consequences. 
- need mandatory regulation , since openai and msoft and meta recklessly released models without safeguards 


Misc, to incorporate in above questions:
- add a column in risk assessment for historical behaviour of deployer? so reckless deployments cause extra scrutiny in future?
- Instead of general-purpose models, we should support domain-specific models. E.g. a model that can do programming should not also be taught bioengineering.
-  part of the risk from AI developments is that it is open-sourced and thereby allows unsafe actors to use the results. So we should encourage that research for capabilities should be closed source unless exemption provided. And research for alignment / safety can be open source.
- have current focus on info gathering , so that we have clear picture of ai landscape for future restrictions, once we know what things are likely dangerous?
    -> cloud providers to make available to gov the billing records for ai supercompute.
    -> companies to preregister trainiruns above a certain supercompute threshold 
   -> if company doing many small training runs that collectively go over threshold, them to report it, and gov to have way to verify it.
