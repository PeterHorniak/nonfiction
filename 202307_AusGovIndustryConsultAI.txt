Read up to page 7.

Misc, not related to a specific question.
- "AI is unique because it can take actions at a speed and scale that would otherwise be impossible." -> This misses the point that AI can combine more knowledge than any human can, leading to insights that humans are unlikely to make. 
- Tracking and potentially restricting computer chips for large training runs seems best approach. Mention this in potential approaches.


Definitions
1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer
and why?
-> The definition for "Machine Learning" more closely applies to "Machine Learning Models" or "AI Models"
-> The definition of "Artificial intelligence (AI)" does not mention agentic behaviour. Maybe this is covered by "operate with varying levels of automation". 


Potential gaps in approaches
2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do
you have suggestions for possible regulatory action to mitigate these risks?
-> <answer>

3. Are there any further non-regulatory initiatives the Australian Government could implement to
support responsible AI practices in Australia? Please describe these and their benefits or
impacts.
-> <answer>

4. Do you have suggestions on coordination of AI governance across government? Please outline
the goals that any coordination mechanisms could achieve and how they could influence the
development and uptake of AI in Australia.
-> <answer>



Responses suitable for Australia
5. Are there any governance measures being taken or considered by other countries (including any
not discussed in this paper) that are relevant, adaptable and desirable for Australia?



Target areas
6. Should different approaches apply to public and private sector use of AI technologies? If so, how
should the approaches differ?
7. How can the Australian Government further support responsible AI practices in its own
agencies?
8. In what circumstances are generic solutions to the risks of AI most valuable? And in what
circumstances are technology-specific solutions better? Please provide some examples.
9. Given the importance of transparency across the AI lifecycle, please share your thoughts on:
a. where and when transparency will be most critical and valuable to mitigate potential AI
risks and to improve public trust and confidence in AI?
b. mandating transparency requirements across the private and public sectors, including
how these requirements could be implemented.
10. Do you have suggestions for:
a. Whether any high-risk AI applications or technologies should be banned completely?
b. Criteria or requirements to identify AI applications or technologies that should be
banned, and in which contexts?
11. What initiatives or government action can increase public trust in AI deployment to encourage
more people to use AI?



Implications and infrastructure
12. How would banning high-risk activities (like social scoring or facial recognition technology in
certain circumstances) impact Australia’s tech sector and our trade and exports with other
countries?
13. What changes (if any) to Australian conformity infrastructure might be required to support
assurance processes to mitigate against potential AI risks?



Risk-based approaches
14. Do you support a risk-based approach for addressing potential AI risks? If not, is there a better
approach?
15. What do you see as the main benefits or limitations of a risk-based approach? How can any
limitations be overcome?
16. Is a risk-based approach better suited to some sectors, AI applications or organisations than
others based on organisation size, AI maturity and resources?
17. What elements should be in a risk-based approach for addressing potential AI risks? Do you
support the elements presented in Attachment C?
18. How can an AI risk-based approach be incorporated into existing assessment frameworks (like
privacy) or risk management processes to streamline and reduce potential duplication?
19. How might a risk-based approach apply to general purpose AI systems, such as large language
models (LLMs) or multimodal foundation models (MFMs)?
20. Should a risk-based approach for responsible AI be a voluntary or self-regulation tool or be
mandated through regulation? And should it apply to:
a. public or private organisations or both?
b. developers or deployers or both?
