*General Comment*
     It's commendable that the Australian Government is proactively consulting on AI risks and regulatory strategies. However, it's crucial to also consider potential catastrophic or existential risks. Policies should not only address near-term AI impacts, but also understand and mitigate longer-term risks.
     The establishment of an AI commission could be a beneficial initial step. This body could oversee the implementation of these strategies, coordinate efforts across different sectors and organizations, and ensure that Australia's approach to AI is comprehensive, effective, and forward-looking.

*Definitions*
1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer
and why?
Answer: 
     The definition of "Artificial intelligence (AI)" does not mention agentic behaviour. Agentic behaviour is a key risk of future models. Could the definition please be expanded to "AI systems are designed to operate with varying levels of automation and agentic behaviour." 

*Potential gaps in approaches*
2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?
Answer: 
     Australia's existing regulatory approaches mainly focus on post-deployment aspects of AI. However, they do not thoroughly address the risks during model training, such as data breaches or unintended model behavior. The legal responsibilities for harms caused by AI systems are also unclear. 
     AI systems, especially those using complex machine learning techniques, can be opaque, making it difficult for regulators and users to understand and trust them. With AI systems becoming more autonomous, ensuring appropriate human control is another area not fully addressed by current regulations.
     To mitigate these risks, several regulatory actions can be considered. Clear legal frameworks assigning responsibility for AI actions could be established. Comprehensive privacy and data security laws for AI systems need to be developed. Regulations should be put in place to prevent bias and discrimination in AI outcomes. Transparency and explainability in AI systems should be promoted, and regulations ensuring appropriate human oversight of autonomous AI systems need to be implemented.
     Furthermore, measures to slow down the development of AI capabilities to allow alignment research to catch up could be considered. This would require a global collaborative effort among leading AI labs and careful monitoring to avoid antitrust behavior. To ensure the traceability of AI-generated content, mandatory rules could be implemented to cryptographically mark such content with relevant information, making it illegal to share unmarked photorealistic AI-generated content on social media. Supercomputer chips designed for AI training runs need to be uniquely identifiable and trackable. A solution is also needed to ensure that dangerous AI models cannot be trained, whether or not they are deployed commercially.

3. Are there any further non-regulatory initiatives the Australian Government could implement to support responsible AI practices in Australia? Please describe these and their benefits or impacts.
Answer:
     The Australian Government has multiple avenues to support responsible AI practices, beyond regulatory interventions:
     1. Attract Global Talent: Offering permanent residency to foreign AI researchers would not only mitigate the risks of overseas AI development but could also stimulate Australia's AI ecosystem. Attracting global talent would spur innovation, promote knowledge transfer, and potentially make Australia a global hub for AI research and development.
     2. Foster an AI-Literate Society: Public understanding of AI is crucial for its responsible use. Initiatives aimed at fostering an AI-literate society, such as educational programs, public discussions, and awareness campaigns, could be implemented. This would empower individuals to make informed decisions about AI use and its implications.
     3. Promote Collaboration: The government could encourage collaborations between academia, industry, and the public sector. Such collaborations could lead to shared best practices, joint research, and the development of innovative AI solutions.
     4. Support Research and Development: Investments in AI research and development, particularly in areas such as transparency, fairness, security, and privacy, would help build more robust and responsible AI systems.
     5. Update AI Ethics Principles: The government could update these principles to address catastrophic and existential risks.
     6. Support International Initiatives: Australia could support international initiatives for safe AI development, such as tracking AI-specific chips, and encourage domestic AI development to use only trackable supercompute from licensed providers. This would lay the groundwork for future regulation of AI hardware, if necessary.
     These initiatives would contribute to a safer, more responsible AI environment in Australia and globally. They would also foster innovation, enhance public understanding of AI, promote collaboration, and potentially make Australia a leader in ethical AI development.

4. Do you have suggestions on coordination of AI governance across government? Please outline the goals that any coordination mechanisms could achieve and how they could influence the development and uptake of AI in Australia.
Answer:
    Coordinating AI governance across the Australian government can be achieved through several mechanisms:
     1. Establish a Centralized Governing Body: This body could oversee AI development, deployment, and management across different agencies. It could also act as a liaison between the government, AI developers, and the public, ensuring an open line of communication and transparency.
     2. Common Standards and Frameworks: Establishing common standards and ethical frameworks for AI use across all government agencies would ensure consistency and accountability. This would also foster trust among the public and stakeholders.
     3. Shared Registers of Resources: Shared registers of resources for AI development, such as training datasets, could be maintained. Linking new AI models to the datasets used for their training would allow better tracking of potential biases.
     4. Public Registers of AI Companies and Safety Ratings: Public registers of AI companies and their safety ratings could be maintained to incentivize safety-conscious practices. These ratings should be carefully defined and regularly updated to avoid becoming victims of Goodhart's law.
     5. Inter-Agency Collaboration: Encourage agencies to collaborate on AI projects, share insights, and learn from each other. This would not only lead to better AI solutions but also create a collaborative environment that fosters innovation.
     6. Education and Training: Provide education and training for government employees on AI ethics, bias, and safety. This would ensure that those working with AI understand the implications and responsibilities.
     The goal of these coordination mechanisms would be to create a consistent, transparent, and accountable AI environment across the government. They would also foster a culture of collaboration and learning, incentivize safety, and ensure that the benefits of AI are realized while mitigating potential risks. This coordinated approach would positively influence the development and uptake of AI in Australia by building trust, promoting innovation, and ensuring responsible AI use.

*Responses suitable for Australia*
5. Are there any governance measures being taken or considered by other countries (including any not discussed in this paper) that are relevant, adaptable and desirable for Australia?
Answer:
     No comment.

*Target areas*
6. Should different approaches apply to public and private sector use of AI technologies? If so, how should the approaches differ?
Answer:
     Yes, different approaches should apply to public and private sector use of AI technologies due to the inherent differences in their objectives, accountability structures, and the nature of the data they handle.
     The private sector, particularly leading AI research organizations, is primarily driven by innovation and competitiveness. They handle a large volume of data, much of which is proprietary, and often operate under commercial confidentiality. Therefore, the focus for private sector AI should be on ethical data handling, privacy protection, and promoting transparency and accountability in AI systems. Regulations and guidelines should be designed to foster innovation while ensuring responsible AI use. The private sector currently controls the most powerful foundational models, and therefore should have the most scrutiny to mitigate catastrophic and existential risk.
     In contrast, the public sector is primarily concerned with public service delivery and policy implementation. They handle sensitive personal data and are directly accountable to the public. Therefore, the focus for public sector AI should be on maintaining public trust, ensuring the highest level of data security, and making AI systems as transparent and understandable as possible. Public sector AI use should be guided by principles that prioritize public interest and welfare.
     Moreover, public sector AI can serve as a role model for responsible AI practices. The government has an opportunity to lead by example in demonstrating best practices for AI use, such as ensuring fairness, transparency, and robustness.
     However, while the approaches should differ, there should also be a level of cooperation and knowledge sharing between the public and private sectors. Joint initiatives, partnerships, and dialogue between the two sectors can lead to better outcomes for all stakeholders.

7. How can the Australian Government further support responsible AI practices in its own agencies?
Answer:
     The Australian Government can support responsible AI practices in its own agencies in several ways:
     1. High-Quality Datasets: The government could provide access to high-quality, well-documented datasets for AI development. The use of synthetic data could help mitigate privacy risks while providing valuable data for training AI models.
     2. Education and Training: Implement comprehensive education and training programs for government employees on AI ethics, potential biases, data privacy, and responsible AI use. This would ensure that those involved in AI projects are well-equipped to handle any ethical or technical challenges that may arise.
     3. Clear Guidelines and Standards: Develop and implement clear guidelines and standards for AI use within government agencies. These should cover areas such as data privacy, transparency, bias mitigation, and ethical AI use. They should also avoid merely providing the appearance of safety via visible procedures, and instead focus on their actual effects.
     4. Inter-Agency Collaboration: Promote collaboration between agencies on AI projects. This could foster a culture of knowledge sharing and best practice dissemination across government departments.
     5. Public Engagement: Engage the public in discussions about how AI is used within government agencies. This could help build public trust and ensure that AI systems are being used in a way that aligns with societal values and expectations.
     6. Pilot Programs: Implement pilot programs to test new AI technologies before full-scale deployment. These programs can help identify any potential issues early and ensure that AI systems are working as intended.
     By implementing these measures, the Australian Government can foster a culture of responsible AI use within its agencies, leading to more effective, transparent, and ethical AI practices.

8. In what circumstances are generic solutions to the risks of AI most valuable? And in what circumstances are technology-specific solutions better? Please provide some examples.
Answer:
     Generic solutions are valuable when we need broader, principle-based guidelines that can apply to a wide range of AI technologies, including those that have not been invented yet or are not fully understood. They are also beneficial in addressing fundamental AI risks that apply regardless of the specific technology involved, such as data privacy, transparency, and accountability. For instance, ethical guidelines and policies concerning data protection can apply to a wide range of AI technologies, from machine learning algorithms to autonomous systems.
     Technology-specific solutions are better when dealing with risks that are unique to a particular AI technology or method. These solutions are usually more technical and targeted, addressing specific issues associated with certain AI applications. 
For example, Reinforcement Learning From Human Feedback (RLHF) presents specific challenges, such as the need for high-quality human feedback and the risk of the AI manipulating the feedback process. In this case, a technology-specific solution could involve developing stricter protocols for human feedback and implementing safeguards to prevent AI manipulation.

9. Given the importance of transparency across the AI lifecycle, please share your thoughts on:
     a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?
     b. mandating transparency requirements across the private and public sectors, including how these requirements could be implemented.
Answer:
     a. Transparency is particularly important in certain aspects:
     Data: Transparency about the data used to train AI models is vital. This includes information about the sources of the data, how it was collected and processed, and any potential biases it may contain. Such transparency can help mitigate the risk of biased AI decisions and improve trust in AI systems.
     Model Development and Testing: Clear documentation of how an AI model was developed, including the algorithms used and choices made during the development process, is essential. Similarly, the results of safety and performance testing should be transparently reported. This can help users understand how the AI system works and how reliable it is, thereby increasing trust and confidence in the system.
     b. Mandating transparency requirements across both private and public sectors can improve the overall trustworthiness of AI systems. However, these requirements must be balanced with considerations for security, privacy, and competitive advantage. Here's how these requirements could be implemented:
     Safeguard Sensitive Information: Transparency should not lead to the release of information that could speed up capabilities development in a way that risks safety or gives rise to misuse. Regulations should be in place to protect sensitive information, such as proprietary algorithms, from being misused.
     Promote Safety Research: Encourage the publication of safety research to foster a broader understanding of AI risks and how to mitigate them. This can be done through incentives or through direct support for such research.
     Control Release of Dual-Use Research: Dual-use research, which can be used for both beneficial and harmful purposes, should be carefully controlled. Policies should be in place to discourage public release of such research without appropriate safeguards.
     Grant Exemptions for Capabilities Research: The publication of capabilities research, which advances AI technology, should only be allowed when an exemption is granted after careful review. This can prevent the reckless dissemination of potentially dangerous AI technology.
     By implementing these measures, the Australian Government can foster a culture of transparency in AI development while still protecting critical information and maintaining a competitive AI sector.

10. Do you have suggestions for:
a. Whether any high-risk AI applications or technologies should be banned completely?
b. Criteria or requirements to identify AI applications or technologies that should be banned, and in which contexts?
Answer:
     a. High-Risk AI Applications or Technologies to be Banned:
     AI models that use a significant amount of compute, potentially more than what was used for GPT-4, should be restricted until there's a scientific consensus that our alignment methods can prevent catastrophic outcomes. Certain alignment methods, like Reinforcement Learning From Human Feedback (RLHF), may not be effective for more powerful models, which could lead to unpredictable and potentially dangerous results.
     Autonomous weapons, particularly those that can make decisions about life and death without human intervention, should be completely banned. These include not only military weapons systems but also those that could be used by law enforcement or border protection. The risks of misuse, accidents, or escalation posed by these systems are too high to be mitigated effectively.
     b. Criteria or Requirements to Identify AI Applications or Technologies to be Banned:
     Potential for Harm: The primary criterion should be the potential for harm. If an AI technology or application has a high risk of causing physical harm, infringing on privacy, or violating human rights, it should be considered for a ban.
     Lack of Effective Oversight: If there's no effective method to provide oversight or control over an AI technology, it should be considered for a ban. This would apply to technologies that are too complex, opaque, or fast-acting for humans to effectively monitor or control.
     Alignment with Human Values: If an AI technology can't be reliably aligned with human values and societal norms, it should be considered for a ban. This would apply to AI systems that make decisions in ways that are incompatible with our ethical and moral standards, or that make decisions in an opaque way that cannot be verified to be aligned.

11. What initiatives or government action can increase public trust in AI deployment to encourage more people to use AI?
Answer:
   The Australian Government can increase public trust in AI deployment and encourage its wider use through several initiatives:
     1. Licensing for AI Systems: Requiring licenses for training or deploying large-scale AI models could help ensure that only qualified and responsible entities are able to develop and use these powerful technologies.
     2. Mandatory AI Insurance: Requiring AI companies to hold insurance specifically for damage caused by their AI models could provide a financial safety net for individuals or businesses harmed by these systems. The amount of insurance should be proportionate to the potential risks posed by the AI models being used.
     3. Social Safety Nets: Providing social safety nets, such as a form of Universal Basic Income, for people displaced by AI advancements could help alleviate public concerns about job losses due to AI, thereby increasing trust in AI technologies.
     4. Public Engagement: Engaging the public in discussions about AI use can help build trust and ensure that AI systems are being developed and used in a manner consistent with societal values and expectations.
     5. Education and Awareness: Implementing educational programs and awareness campaigns about AI can help increase public understanding of this technology, its benefits, and its risks. An informed public is more likely to trust and use AI technologies.
     6. Transparency: Promoting transparency in AI development and use can also foster trust. This could include clear communication about how AI systems work, what data they use, and how they make decisions.
     7. Robust Testing and Auditing: Implementing robust testing and auditing procedures for AI systems can help ensure they're reliable and safe, which can increase public trust.
     These initiatives could contribute to a more responsible and trustworthy AI environment, thereby encouraging more people to use AI.

*Implications and infrastructure*
12. How would banning high-risk activities (like social scoring or facial recognition technology in certain circumstances) impact Australia’s tech sector and our trade and exports with other countries?
Answer:
     Banning high-risk activities could have both positive and negative impacts. On the positive side, these bans could:
     1. Elevate Australia's Reputation: Australia could be seen as a global leader in ethical AI practices. This could attract ethical investors and businesses, potentially spurring growth in the tech sector.
     2. Promote Trust: Banning high-risk AI activities could increase public trust in AI technologies, potentially driving adoption and innovation in safer AI applications.
     3. Attract Talent: This could make Australia a desirable place for AI researchers who are concerned about the ethical implications of their work, attracting top talent to the country's tech sector.
     Potential negative impacts are:
     1. Limit Innovation: Such bans could limit innovation in the tech sector. Companies may move to other countries with less stringent regulations, leading to a loss of business and talent.
     2. Reduce Competitiveness: Australia might be less competitive in the global AI market, potentially impacting trade and exports. Some international partners may be reluctant to engage with a country that has strict bans on certain AI technologies.
     3. Influence Trade Relations: Depending on how these bans are viewed by other nations, they could either strengthen or weaken trade relations. Countries that value ethical AI practices may see Australia as a preferred trade partner, while others may see these bans as a barrier to trade.

13. What changes (if any) to Australian conformity infrastructure might be required to support assurance processes to mitigate against potential AI risks?
Answer:
     To mitigate potential AI risks, several changes might be needed in Australian conformity infrastructure:
     1. Standardized Measures for AI Power: Standardized measures to determine the power of AI models would need to be established. This could be based on various factors such as the number of parameters, the amount of compute used for training, or other relevant measures. Consensus on these standards would need to be achieved through consultation with industry and academic experts.
     2. Licensing Scheme for AI Training and Operation: A new licensing scheme might be necessary for organizations intending to start training runs or operate models beyond a certain threshold of power or risk. This would ensure that only qualified and responsible entities are developing and using high-powered AI systems.
     3. Cloud Provider Compliance: Cloud providers should be required to make billing records for AI supercompute usage available to regulatory and law enforcement agencies. This would allow these agencies to track and monitor the use of AI compute resources.
     4. Government Register for AI Training Runs: Companies could be required to preregister AI training runs above a certain supercompute threshold in a government register. To ensure compliance, third-party auditors could be given access to verify the usage of supercompute resources.
     5. Efficient Communications for Law Enforcement: Systems could be put in place to allow efficient communication between law enforcement and cloud/compute providers. This would enable quick action in cases where AI models are being used maliciously, such as in political misinformation campaigns.
     6. Provenance for AI Models: Measures should be implemented to ensure the provenance of AI models. This would help prevent the dissemination of manipulated models, as highlighted by the PoisonGPT research example, where a model was modified to spread misinformation. Provenance tracking would ensure that AI models are correctly attributed and their origins traceable.
     By implementing these changes, Australia can strengthen its conformity infrastructure to better manage the risks associated with AI technologies and ensure responsible AI practices across the board.

*Risk-based approaches*
14. Do you support a risk-based approach for addressing potential AI risks? If not, is there a better approach?
Answer:
     Yes.

15. What do you see as the main benefits or limitations of a risk-based approach? How can any limitations be overcome?
Answer:
  The main benefits of a risk-based approach to AI governance include its flexibility and its focus on the most significant issues. This approach allows regulations and controls to be tailored to the specific risks posed by different AI technologies or applications. It allows us to concentrate our resources on the areas of greatest concern.
     However, there are also several limitations:
     1. Theoretical Risks: Some of the greatest risks from AI are currently theoretical and might not have clear identifying features. This makes it challenging to assess these risks accurately and to develop effective mitigation strategies.
     2. Variable Risk Tolerance: A risk-based approach often assumes a uniform risk tolerance across all individuals and groups. However, different people and groups may have different levels of risk tolerance. For example, patients with a terminal disease might be more willing to accept risks associated with an experimental treatment.
     3. Short-Term vs. Long-Term Risks: Some risks might be low in the short term but become much higher over the long term. A risk-based approach needs to take both short-term and long-term risks into account, which can be challenging.
     To overcome these limitations, several strategies can be employed:
     1. Regular Risk Reviews: Risks should be reviewed regularly to account for new information, changes in context, and the emergence of new risks. This would ensure that the risk assessments remain accurate and relevant over time.
     2. Stakeholder Input: Different stakeholders should be involved in the risk assessment process to account for different perspectives and risk tolerances. This could include AI developers, users, and people who could be affected by the AI system.
     3. Precautionary Approach: For theoretical or long-term risks that are difficult to quantify, a precautionary approach might be appropriate. This could involve implementing safeguards and controls even in the absence of clear evidence of harm, to ensure that potential risks are mitigated.

16. Is a risk-based approach better suited to some sectors, AI applications or organisations than others based on organisation size, AI maturity and resources?
Answer:
     Yes.
     1. Organizational Size: Larger organizations with more resources may be better equipped to implement a risk-based approach. They have the capacity to conduct comprehensive risk assessments, implement sophisticated risk management strategies, and continuously monitor and adjust their approach based on changing risks. Smaller organizations, on the other hand, might find a risk-based approach more challenging due to limited resources, but could still benefit from a simplified or scaled-down version of the approach.
     2. AI Maturity: Organizations with more mature AI technologies may also be better suited to a risk-based approach. They have a better understanding of the capabilities and limitations of their AI systems, which enables them to more accurately assess and manage risks. In contrast, organizations with less mature AI technologies may face greater uncertainty, making risk assessment and management more challenging.
     3. Sector-Specific Considerations: A risk-based approach may be particularly beneficial in sectors where the potential risks of AI are high. For example, in healthcare, where AI decisions can directly impact patient health and safety, a risk-based approach can focus on ensuring the greatest potential harms are not enacted.
     4. Resource Availability: Organizations with more resources can dedicate the necessary time, personnel, and financial resources to managing AI risks. This includes investing in risk assessment tools, training for staff, and continuous monitoring and adjustment of AI systems to manage risks effectively.

17. What elements should be in a risk-based approach for addressing potential AI risks? Do you support the elements presented in Attachment C?
Answer:
     The elements presented in Attachment C form a solid base for a risk-based approach to addressing potential AI risks. However, some additional elements could further strengthen this approach:
     1. Licensing Scheme for AI Training and Deployment: A licensing scheme could be implemented for the training and/or deployment of AI models beyond a certain threshold. This would help ensure that only responsible and qualified entities are developing and using high-powered AI models.
     2. Consideration of Past Behavior: The past behavior of companies and their developers should be considered when assessing risk. This includes adherence to ethical guidelines, previous incidents or violations, and overall safety culture. A safety culture can take a long time to establish and is a crucial element of responsible AI use. One example is OpenAI, whose safety culture was so lax that many key employees left to found the company Anthropic, with a primary goal being a proper safety culture.
     3. Risk Levels for General-Purpose vs. Domain-Specific Models: Risk assessments should take into account the type of AI model being used. General-purpose models, which are trained on a wide range of data and can be used in many different contexts, might be more risky than domain-specific models that are trained and used in a narrower context. 
     4. Continuous Monitoring and Adjustment: A risk-based approach should include mechanisms for continuous monitoring of AI systems and adjusting risk assessments and mitigation strategies as new information becomes available or as the context changes.
     Incorporating these additional elements into a risk-based approach could provide a more comprehensive and effective way to manage the potential risks of AI.

18. How can an AI risk-based approach be incorporated into existing assessment frameworks (like privacy) or risk management processes to streamline and reduce potential duplication?
Answer:
     Incorporating an AI risk-based approach can be done through a few key steps, with the aim of streamlining procedures and reducing potential duplication:
     1. Integration with Existing Risk Management Processes: AI risks can be integrated into an organization's existing risk management processes. This would involve identifying where AI risks overlap with other types of risks, such as cybersecurity, privacy, and operational risks, and addressing them within the same risk management framework. This could streamline processes and ensure a consistent approach to risk management across the organization.
     2. Alignment with Existing Assessment Frameworks: AI risk assessment should align with existing assessment frameworks, such as those for privacy or data protection. For instance, the process of assessing the privacy risks of an AI system could be incorporated into an organization's existing privacy impact assessment (PIA) processes. This would allow for the identification and mitigation of privacy risks specific to AI within an established framework.
     3. Shared Responsibility: Responsibility for AI risk management should be shared across different roles and departments within an organization, rather than being the sole responsibility of a specific AI or data science team. This could ensure that AI risks are considered as part of all relevant decision-making processes and reduce the risk of duplication. Conversely, having shared responsibility may cause no one person or team to own the AI risk.

19. How might a risk-based approach apply to general purpose AI systems, such as large language models (LLMs) or multimodal foundation models (MFMs)?
Answer:
     A risk-based approach would involve several considerations:
     1. Higher Risk Levels: General-purpose AI systems, due to their broad capabilities and potential applications, inherently pose higher risks compared to domain-specific systems. They have the potential to generate outputs or actions that could be harmful or inappropriate in various contexts. Therefore, they should be subject to a higher level of scrutiny and control under a risk-based approach.
     2. Robust Testing: Given the wide range of potential outputs from general-purpose AI systems, robust testing is crucial. This includes testing the system's responses to a wide variety of inputs, across different domains and scenarios, to identify potential risks and mitigate them before deployment. Testing should consider how the system might behave outside the training distribution, i.e. in the real world.
     3. Transparency and Explainability: It's important to make the workings of these models as transparent and understandable as possible. This can help users, regulators, and the public to understand the potential risks and benefits of these systems, and to make informed decisions about their use.
     4. User Education: Users of these systems should be educated about their potential risks and limitations, and how to use them responsibly. This can help to prevent misuse and to ensure that users are prepared to handle any unexpected outputs or actions from the system.

20. Should a risk-based approach for responsible AI be a voluntary or self-regulation tool or be mandated through regulation? And should it apply to:
a. public or private organisations or both?
b. developers or deployers or both?
Answer:
     A risk-based approach for responsible AI should be mandated through regulation, rather than being left as a voluntary or self-regulation tool. This is because the potential risks associated with AI are significant and could impact society in profound ways. Relying on voluntary adherence or self-regulation may not be sufficient to ensure that all organizations take the necessary precautions.
     The mandated approach should apply to both public and private organizations. The potential risks of AI are not confined to one sector or type of organization, and thus all organizations that develop or use AI should be held to the same standards. Public organizations, as representatives of the state, have a particular responsibility to use AI responsibly. Private organizations, which often lead the way in AI development, also have a significant responsibility to ensure that their technologies are safe and beneficial.
     Both developers and deployers of AI should be covered by the mandated risk-based approach. Developers have a responsibility to ensure that the AI systems they create are safe, reliable, and ethical. Deployers, who often have direct control over how AI systems are used and who may use them in ways not anticipated by the developers, also have a responsibility to ensure that these systems are used responsibly.
     The argument for mandatory regulation is supported by past instances where AI companies have struggled with self-regulation. There have been notable cases where companies like OpenAI, Microsoft, and Meta have faced challenges in managing the risks associated with their AI technologies. These examples illustrate the limitations of self-regulation and underline the need for a more structured, regulatory approach.
