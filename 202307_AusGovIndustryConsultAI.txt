Read up to page 24.

Misc, not related to a specific question.
- "AI is unique because it can take actions at a speed and scale that would otherwise be impossible." -> This misses the point that AI can combine more knowledge than any human can, leading to insights that humans are unlikely to make. 
- Tracking and potentially restricting computer chips for large training runs seems best approach. Mention this in potential approaches.
- Offer permanent residency to any AI researchers in countries that we expect to not follow multinational agreements? Would give us more control over research done onshore, and reduce the researchers available to those other countries. Would be worth it even ignoring the economic benefits of Australia being an AI hub.
- need mandatory regulation , since openai and msoft and meta recklessly released models without safeguards 
- AustEthicPrinciples
     #1 with notkilleveryoneism
     #2 aligns with alignment and
      #6 tag every content generated by ai and require downstream applications to conform to it?
- AI safety newsletter #12 has relevant ideas 
- page 24 mentions competition issues. We need to be reducing competition at the most powerful models, to prevent a race.

Definitions
1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer
and why?
-> The definition for "Machine Learning" more closely applies to "Machine Learning Models" or "AI Models"
-> The definition of "Artificial intelligence (AI)" does not mention agentic behaviour. Maybe this is covered by "operate with varying levels of automation". 


Potential gaps in approaches
2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?
-> <answer>

3. Are there any further non-regulatory initiatives the Australian Government could implement to
support responsible AI practices in Australia? Please describe these and their benefits or
impacts.
-> <answer>

4. Do you have suggestions on coordination of AI governance across government? Please outline
the goals that any coordination mechanisms could achieve and how they could influence the
development and uptake of AI in Australia.
-> <answer>



Responses suitable for Australia
5. Are there any governance measures being taken or considered by other countries (including any not discussed in this paper) that are relevant, adaptable and desirable for Australia?



Target areas
6. Should different approaches apply to public and private sector use of AI technologies? If so, how should the approaches differ?

7. How can the Australian Government further support responsible AI practices in its own
agencies?

8. In what circumstances are generic solutions to the risks of AI most valuable? And in what circumstances are technology-specific solutions better? Please provide some examples.

9. Given the importance of transparency across the AI lifecycle, please share your thoughts on:
a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?
-> training run details to compare model sizes
b. mandating transparency requirements across the private and public sectors, including
how these requirements could be implemented.

10. Do you have suggestions for:
a. Whether any high-risk AI applications or technologies should be banned completely?
-> models of bigger size than gpt4
-> autonomous weapon use in police and border protection 
-> pentesting
-> training methods that are whackamole by design, eg RLHF
b. Criteria or requirements to identify AI applications or technologies that should be
banned, and in which contexts?
-> threshold of it becoming agentic. Need scientific consensus that a given tech will not become agentic.
-> 

11. What initiatives or government action can increase public trust in AI deployment to encourage more people to use AI?
-> require insurance for ai companies , with payouts going to victims 



Implications and infrastructure
12. How would banning high-risk activities (like social scoring or facial recognition technology in certain circumstances) impact Australia’s tech sector and our trade and exports with other countries?
-> 
13. What changes (if any) to Australian conformity infrastructure might be required to support assurance processes to mitigate against potential AI risks?
-> 



Risk-based approaches
14. Do you support a risk-based approach for addressing potential AI risks? If not, is there a better approach?
-> combine with required insurances, let private insurance companies do additional diligence
15. What do you see as the main benefits or limitations of a risk-based approach? How can any limitations be overcome?
-> thresholds can be arbitrary 
-> ai devs might not know what category their stuff falls under

16. Is a risk-based approach better suited to some sectors, AI applications or organisations than others based on organisation size, AI maturity and resources?
->

17. What elements should be in a risk-based approach for addressing potential AI risks? Do you support the elements presented in Attachment C?

18. How can an AI risk-based approach be incorporated into existing assessment frameworks (like
privacy) or risk management processes to streamline and reduce potential duplication?

19. How might a risk-based approach apply to general purpose AI systems, such as large language models (LLMs) or multimodal foundation models (MFMs)?
-> hard to fo transparency on them due to inscrutable floating point matrices

20. Should a risk-based approach for responsible AI be a voluntary or self-regulation tool or be mandated through regulation? And should it apply to:
a. public or private organisations or both?
b. developers or deployers or both?
-> mandated, apply to all. Developers can have cruddy security and have their tech stolen and misused , and then be responsible for consequences. 