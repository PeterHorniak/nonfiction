*General Comment*
It is good that the government is publicly consulting on this issue and is aware of the near-term risks. It is concerning that the discussion paper does not mention catastrophic nor existential risks. A mature AI strategy would take such risks into account and focus on learning more about them and mitigating them. An AI commission would be a good initial step.

*Definitions*
1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer
and why?
Answer: 
     The definition of "Artificial intelligence (AI)" does not mention agentic behaviour. Agentic behaviour is a key risk of future models. Could the definition please be expanded to "AI systems are designed to operate with varying levels of automation and agentic behaviour." 

*Potential gaps in approaches*
2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?
Answer: 
     Australia's existing regulatory approaches mainly focus on post-deployment aspects of AI. However, they do not thoroughly address the risks during model training, such as data breaches or unintended model behavior. The legal responsibilities for harms caused by AI systems are also unclear. 
     AI systems, especially those using complex machine learning techniques, can be opaque, making it difficult for regulators and users to understand and trust them. With AI systems becoming more autonomous, ensuring appropriate human control is another area not fully addressed by current regulations.
     To mitigate these risks, several regulatory actions can be considered. Clear legal frameworks assigning responsibility for AI actions could be established. Comprehensive privacy and data security laws for AI systems need to be developed. Regulations should be put in place to prevent bias and discrimination in AI outcomes. Transparency and explainability in AI systems should be promoted, and regulations ensuring appropriate human oversight of autonomous AI systems need to be implemented.
     Furthermore, measures to slow down the development of AI capabilities to allow alignment research to catch up could be considered. This would require a global collaborative effort among leading AI labs and careful monitoring to avoid antitrust behavior. To ensure the traceability of AI-generated content, mandatory rules could be implemented to cryptographically mark such content with relevant information, making it illegal to share unmarked photorealistic AI-generated content on social media. Supercomputer chips designed for AI training runs need to be uniquely identifiable and trackable. A solution is also needed to ensure that dangerous AI models cannot be trained, whether or not they are deployed commercially.

3. Are there any further non-regulatory initiatives the Australian Government could implement to support responsible AI practices in Australia? Please describe these and their benefits or impacts.
Answer:
     The Australian Government has multiple avenues to support responsible AI practices, beyond regulatory interventions:
     1. Attract Global Talent: Offering permanent residency to foreign AI researchers would not only mitigate the risks of overseas AI development but could also stimulate Australia's AI ecosystem. Attracting global talent would spur innovation, promote knowledge transfer, and potentially make Australia a global hub for AI research and development.
     2. Foster an AI-Literate Society: Public understanding of AI is crucial for its responsible use. Initiatives aimed at fostering an AI-literate society, such as educational programs, public discussions, and awareness campaigns, could be implemented. This would empower individuals to make informed decisions about AI use and its implications.
     3. Promote Collaboration: The government could encourage collaborations between academia, industry, and the public sector. Such collaborations could lead to shared best practices, joint research, and the development of innovative AI solutions.
     4. Support Research and Development: Investments in AI research and development, particularly in areas such as transparency, fairness, security, and privacy, would help build more robust and responsible AI systems.
     5. Update AI Ethics Principles: The government could update these principles to address catastrophic and existential risks.
     6. Support International Initiatives: Australia could support international initiatives for safe AI development, such as tracking AI-specific chips, and encourage domestic AI development to use only trackable supercompute from licensed providers. This would lay the groundwork for future regulation of AI hardware, if necessary.
     These initiatives would contribute to a safer, more responsible AI environment in Australia and globally. They would also foster innovation, enhance public understanding of AI, promote collaboration, and potentially make Australia a leader in ethical AI development.

4. Do you have suggestions on coordination of AI governance across government? Please outline the goals that any coordination mechanisms could achieve and how they could influence the development and uptake of AI in Australia.
Answer:
    Coordinating AI governance across the Australian government can be achieved through several mechanisms:
     1. Establish a Centralized Governing Body: This body could oversee AI development, deployment, and management across different agencies. It could also act as a liaison between the government, AI developers, and the public, ensuring an open line of communication and transparency.
     2. Common Standards and Frameworks: Establishing common standards and ethical frameworks for AI use across all government agencies would ensure consistency and accountability. This would also foster trust among the public and stakeholders.
     3. Shared Registers of Resources: Shared registers of resources for AI development, such as training datasets, could be maintained. Linking new AI models to the datasets used for their training would allow better tracking of potential biases.
     4. Public Registers of AI Companies and Safety Ratings: Public registers of AI companies and their safety ratings could be maintained to incentivize safety-conscious practices. These ratings should be carefully defined and regularly updated to avoid becoming victims of Goodhart's law.
     5. Inter-Agency Collaboration: Encourage agencies to collaborate on AI projects, share insights, and learn from each other. This would not only lead to better AI solutions but also create a collaborative environment that fosters innovation.
     6. Education and Training: Provide education and training for government employees on AI ethics, bias, and safety. This would ensure that those working with AI understand the implications and responsibilities.
     The goal of these coordination mechanisms would be to create a consistent, transparent, and accountable AI environment across the government. They would also foster a culture of collaboration and learning, incentivize safety, and ensure that the benefits of AI are realized while mitigating potential risks. This coordinated approach would positively influence the development and uptake of AI in Australia by building trust, promoting innovation, and ensuring responsible AI use.

*Responses suitable for Australia*
5. Are there any governance measures being taken or considered by other countries (including any not discussed in this paper) that are relevant, adaptable and desirable for Australia?
Answer:
     No comment.

*Target areas*
6. Should different approaches apply to public and private sector use of AI technologies? If so, how should the approaches differ?
Answer:
     Yes. The majority of leading AI research is currently done by the private sector. This is where most risk exists, and should therefore be the main focus of risk mitigation. 
     TODO: describe how approach should differ.

7. How can the Australian Government further support responsible AI practices in its own agencies?
Answer:
     Make datasets available for AI development that are already high-quality and documented. These can be made of synthetic data to avoid privacy risk. This will let AI developers spend less time on data-gathering.

8. In what circumstances are generic solutions to the risks of AI most valuable? And in what circumstances are technology-specific solutions better? Please provide some examples.
Answer:
     Generic solutions are most valuable when we want the solution to apply to technologies that have not been created yet or are not widely understood. 
     Technology-specific solutions are better when we know a particular approach to AI development is dangerous. For example, using Reinforcement Learning From Human Feedback (RLHF) relies on humans ranking outputs of AI models, and thus requires the human reviewers to understand which outputs are better or worse. This approach breaks down when quality human feedback does not scale, or when the model is intelligent enough to manipulate the feedback process.
     TODO: add example for generic solution.

9. Given the importance of transparency across the AI lifecycle, please share your thoughts on:
     a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?
     b. mandating transparency requirements across the private and public sectors, including how these requirements could be implemented.
Answer:
     For a), transparency will be most critical when describing the data used to train models, and the results of safety testing on the models.
     For b), transparency requirements should not lead to releasing info that can speed up capabilities development across the industry. Publicly available source code or architecture allows bad actors to use them in unsafe ways. The focus should be on making safety research available publicly, dual-use research discouraged from public release, and capabilities research only published when an exemption is granted.

10. Do you have suggestions for:
a. Whether any high-risk AI applications or technologies should be banned completely?
b. Criteria or requirements to identify AI applications or technologies that should be banned, and in which contexts?
Answer:
     There should be a ban on AI models that use more compute than GPT-4, until the scientific consensus is that our alignment methods are sufficient to prevent catastrophic outcomes. An example of an alignment method that is unlikely to work on more powerful models is Reinforcement Learning From Human Feedback (RLHF).
     There should also be a ban on autonomous weapons used by police or border protection.

11. What initiatives or government action can increase public trust in AI deployment to encourage more people to use AI?
Answer:
     Require licensing for training or deploying models beyond a certain size in supercompute. 
     Require AI companies to get insurance that is specifically for damage caused by their AI models. The amount of insurance should be proportional to the size of the models being used.
     Provide a safety net for people who lose their jobs due to AI advancements, for example a form of Universal Basic Income.

*Implications and infrastructure*
12. How would banning high-risk activities (like social scoring or facial recognition technology in certain circumstances) impact Australia’s tech sector and our trade and exports with other countries?
Answer:
     No comment.

13. What changes (if any) to Australian conformity infrastructure might be required to support assurance processes to mitigate against potential AI risks?
Answer:
     Standardised measures will need to be agreed on to determine the power of AI models. For example, should it be based on number of parameters, or number of GPUs used to train them.
     A new licensing scheme will be needed to determine which organisations are allowed to start training runs or operate models beyond a certain threshold.
     Cloud providers should make billing records for AI supercompute available to law enforcement. 
     Companies should have a government register available to preregister training runs above a given supercompute threshold. If a company is using that supercompute across multiple smaller training runs, then third-party auditors should have access to verify this.
     Law enforcement should have efficient communications available to cloud and compute providers for when malicious models need to be turned off. For example, when a political misinformation campaign is detected.
     Models need provenance to ensure that they have not been manipulated to answer specific prompts differently, and then shared on a model hub under the guise of being the original model (https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/).

*Risk-based approaches*
14. Do you support a risk-based approach for addressing potential AI risks? If not, is there a better approach?
Answer:
     Yes.

15. What do you see as the main benefits or limitations of a risk-based approach? How can any limitations be overcome?
Answer:
     Some of the greatest risks are currently theoretical and might not have clear identifying features.
     A risk-based approach assumes all impacted people have the same risk tolerance. There could be some groups with a higher risk tolerance. For example, patients with a terminal disease may be willing to undergo robotic surgery that has undergone less testing than is required for the general public.
    Some risks can be low in the short-term, and become a higher risk over the longer-term. The approach would need to either combine both in the initial assessment, or periodically review the assigned risk.

16. Is a risk-based approach better suited to some sectors, AI applications or organisations than others based on organisation size, AI maturity and resources?
Answer:
     No comment.

17. What elements should be in a risk-based approach for addressing potential AI risks? Do you support the elements presented in Attachment C?
Answer:
     I believe the elements in Attachment C are necessary yet not sufficient. 
     There needs to be a licensing scheme for allowing AI models beyond a certain threshold to be trained and/or deployed. 
     There also needs to be consideration of the past behaviour of companies and their developers, since a safety culture can take a long time to establish.
     For a given use case, general-purpose models should have a higher risk level than domain-specific models. In the use case of generative AI in educational settings to assess the performance of teachers and students, a model that was only trained on educational materials relevant to a specific field, is less risky than a model trained on all educational materials that are publicly available.

18. How can an AI risk-based approach be incorporated into existing assessment frameworks (like privacy) or risk management processes to streamline and reduce potential duplication?
Answer:
     No comment.

19. How might a risk-based approach apply to general purpose AI systems, such as large language models (LLMs) or multimodal foundation models (MFMs)?
Answer:
     General purpose AI systems are more difficult to align due to their enhanced capabilities. They are also more risky to deploy, because they are not limited to just one risky domain. General purpose AI systems should be considered to have a higher risk level, or conversely domain-specific systems should be considered to have a lower risk level.

20. Should a risk-based approach for responsible AI be a voluntary or self-regulation tool or be mandated through regulation? And should it apply to:
a. public or private organisations or both?
b. developers or deployers or both?
Answer:
     It should be mandated, apply to both public and private organisations, and apply to both developers and deployers.
     We have already seen examples of AI companies failing at self-regulation: OpenAI publishing the architecture for GPT-3, Microsoft deploying Bing Chat aka Sydney in an obviously unaligned state, and Meta losing the LLaMa model on 4chan. 

Misc, to incorporate in above questions:
- Add links to relevant papers, e.g. https://arxiv.org/abs/2303.11341
- Review AI safety newsletter #12, on US request for comments on same topic.
