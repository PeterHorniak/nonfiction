*Definitions*
1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer
and why?
Answer: 
The definition of "Artificial intelligence (AI)" does not mention agentic behaviour. Agentic behaviour is a key risk of future models. Could the definition please be expanded to "AI systems are designed to operate with
varying levels of automation and agentic behaviour." 



*Potential gaps in approaches*
2. What potential risks from AI are not covered by Australia’s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?
Answer: 
Australia's existing regulatory approaches focus on what happens after a model is deployed. These approaches do not address the risks that exist while training a model. It is possible for model to cause harm before deployment, for example by being stolen, leaked, or by it choosing to escape.
Another risk is that currently Australia cannot stop the training of a model that has a high probability of being deployed in future and then causing harm. One approach to mitigate this risk is to require computer chips designed for AI training runs to be uniquely identifiable and trackable.


Misc, not related to a specific question.
- Offer permanent residency to any AI researchers in countries that we expect to not follow multinational agreements? Would give us more control over research done onshore, and reduce the researchers available to those other countries. Would be worth it even ignoring the economic benefits of Australia being an AI hub.
- need mandatory regulation , since openai and msoft and meta recklessly released models without safeguards 
- AustEthicPrinciples
     #1 with notkilleveryoneism
     #2 aligns with alignment and
      #6 tag every content generated by ai and require downstream applications to conform to it?
- AI safety newsletter #12 has relevant ideas 
- page 24 mentions competition issues. We need to be reducing competition at the most powerful models, to prevent a race.
- paper is focussed on deployed models, does not account for risk of just training a frontier model
- add a column in risk assessment for historical behaviour of deployer? so reckless deployments cause extra scrutiny in future?
- Instead of general-purpose models, we should support domain-specific models. E.g. a model that can do programming should not also be taught bioengineering.

3. Are there any further non-regulatory initiatives the Australian Government could implement to support responsible AI practices in Australia? Please describe these and their benefits or
impacts.
-> Visas for foreign AI researchers. Might even slightly reduce responsible ai practices in Australia, yet could prevent extremely irresponsible AI practices overseas
-> support intl initiatives for tracking compute
-> only procure ai services with trackable compute
Supporting international initiatives to track computer chips designed for AI training runs, so that future regulation of them becomes possible.

4. Do you have suggestions on coordination of AI governance across government? Please outline the goals that any coordination mechanisms could achieve and how they could influence the development and uptake of AI in Australia.
-> Have shared registers of training datasets. Goal is to quickly identify info common to multiple ai models.
-> public registers of ai companies and their safety ratings, to reward thr most safety-conscious. Would need a way to prevent goodharting for safety theatrr.



*Responses suitable for Australia*
5. Are there any governance measures being taken or considered by other countries (including any not discussed in this paper) that are relevant, adaptable and desirable for Australia?
-> outright banning per EU seems correct when uncertainty about catastrophic harms.


*Target areas*
6. Should different approaches apply to public and private sector use of AI technologies? If so, how should the approaches differ?
-> private sector does most research, therefore is more risky and needs most focus on security 
-> private sector also more slack on security so needs more focus



7. How can the Australian Government further support responsible AI practices in its own
agencies?
-> Have datasets available that are available for ai development without restrictions on use. Could be synthetic data. So devs can skip the data gathering stage.


8. In what circumstances are generic solutions to the risks of AI most valuable? And in what circumstances are technology-specific solutions better? Please provide some examples.
-> techspecific better when we know particular approach is dangerous , eg rlhf 
-> generic better when we don't know specific thing to regulate yet.



9. Given the importance of transparency across the AI lifecycle, please share your thoughts on:
a. where and when transparency will be most critical and valuable to mitigate potential AI risks and to improve public trust and confidence in AI?
-> training run details to compare model sizes
-> redteaming results to be public

b. mandating transparency requirements across the private and public sectors, including how these requirements could be implemented.
-> 



10. Do you have suggestions for:
a. Whether any high-risk AI applications or technologies should be banned completely?
-> models of bigger size than gpt4
-> autonomous weapon use in police and border protection 
-> pentesting
-> training methods that are whackamole by design, eg RLHF
b. Criteria or requirements to identify AI applications or technologies that should be
banned, and in which contexts?
-> threshold of it becoming agentic. Need scientific consensus that a given tech will not become agentic.


11. What initiatives or government action can increase public trust in AI deployment to encourage more people to use AI?
-> require insurance for ai companies , with payouts going to victims 
-> ubi, so even if all lose jobs, can still be better off


*Implications and infrastructure*
12. How would banning high-risk activities (like social scoring or facial recognition technology in certain circumstances) impact Australia’s tech sector and our trade and exports with other countries?
-> cannot profit off selling them
-> would make devs focus on other areas that might even be more profitable because the uses wont be banned later on.
13. What changes (if any) to Australian conformity infrastructure might be required to support assurance processes to mitigate against potential AI risks?
-> todo: lkp what is conformity infra



*Risk-based approaches*
14. Do you support a risk-based approach for addressing potential AI risks? If not, is there a better approach?
-> combine with required insurances, let private insurance companies do additional diligence


15. What do you see as the main benefits or limitations of a risk-based approach? How can any limitations be overcome?
-> thresholds can be arbitrary 
-> ai devs might not know what category their stuff falls under
-> limitation is that something can be shortterm low risk and longterm high risk. What time horizons are for the assessment?
-> when there is uncertainty, does it default as higher risk?


16. Is a risk-based approach better suited to some sectors, AI applications or organisations than others based on organisation size, AI maturity and resources?
->

17. What elements should be in a risk-based approach for addressing potential AI risks? Do you support the elements presented in Attachment C?

18. How can an AI risk-based approach be incorporated into existing assessment frameworks (like privacy) or risk management processes to streamline and reduce potential duplication?

19. How might a risk-based approach apply to general purpose AI systems, such as large language models (LLMs) or multimodal foundation models (MFMs)?
-> hard to fo transparency on them due to inscrutable floating point matrices

20. Should a risk-based approach for responsible AI be a voluntary or self-regulation tool or be mandated through regulation? And should it apply to:
a. public or private organisations or both?
b. developers or deployers or both?
-> mandated, apply to all. Developers can have cruddy security and have their tech stolen and misused , and then be responsible for consequences. 
