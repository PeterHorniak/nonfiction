MAILTO: brian@skeptoid.com; Please include as much supporting information as you have. Links to YouTube videos will not be watched, so write up a summary instead.
SUBJECT: Podcast Episode Request - AI Extinction Risk

Dear Brian,

I am impressed at how you take on kooky-sounding topics and give them a fair hearing. There’s a topic I’ve been looking into for the past year, and would love your sceptical perspective on: AI extinction risk.

The claim is that aligning artificial intelligence to human goals is extremely difficult and needs to be prioritised over making AIs more capable. I find the claim logical, and supported by recent issues found in AIs. I am interested how you interpret the research and what conclusion you come to.

Now is a particularly important time to know the truth about this. At least #% of the AI research field (which is broader than the AI alignment field) believes AGI will come within ## years (linkie). The US government is considering how to regulate AI development. If AI alignment is as difficult as suggested, then we need to strongly regulate its development. Conversely if alignment is easy, then we risk losing the innovation and wealth from AI.

https://www.safe.ai/statement-on-ai-risk

Kind regards,
Peter
