SUBJECT: Podcast Episode Request - AI Extinction Risk

Dear Brian,

I hope this email finds you well. I am a long-time listener of Skeptoid. Your approach to investigating and presenting contentious topics has several times changed my beliefs. Today I am reaching out about a topic that's been on my mind for the past year: AI extinction risk.

The claim for AI extinction risk is that aligning artificial intelligence with human goals is an incredibly challenging task and should take precedence over enhancing AI capabilities. I find this claim logically sound, and supported by recent unexpected issues that have arisen with AI systems. I am curious to know your interpretation of the research on this topic and the conclusion you would draw.

Now is a crucial time for people to be informed on AI extinction risk. According to a majority of AI researchers, superforecasters, and a prominent AI development model, "transformative AI is most likely in the period between about 10 and 40 years from now" (source: https://asteriskmag.com/issues/03/through-a-glass-darkly). This is concerning, as most notable AI scientists and many public figures have signed the statement "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war." (source: https://www.safe.ai/statement-on-ai-risk).

I believe that your skeptic perspective would help inform the public. Thank you for your time and consideration.

Kind regards,
Peter


Here are several sources for supporting information. If these aren't in the ideal format, or if you prefer that I summarise all points on one page, I am happy to do so.

Article summarizing attempts to forecast AI progress. 
The sources are surveys from 2016 and 2021 of AI experts, a forecasting tournament called Metaculus, and a report by the charity Open Philanthropy.
(https://asteriskmag.com/issues/03/through-a-glass-darkly).

Statement by AI experts and notable public figures that warns of AI extinction risk. 
The statement is "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."
(https://www.safe.ai/statement-on-ai-risk)

Formulating the AI Doom Argument for Analytic Philosophers. 
This is a short structured argument for extinction risk.
(https://www.lesswrong.com/posts/TC7GhGaKFqTtQH9Aq/formulating-the-ai-doom-argument-for-analytic-philosophers)

Why AI Will Save the World by Marc Andreessen.
This is a popular article that makes the case against AI extinction risk. Its main claim is that AI cannot have separate goals from ourselves. The author is a well-known entrepreneur and software engineer.
(https://a16z.com/2023/06/06/ai-will-save-the-world/)

Human Compatible: Artificial Intelligence and the Problem of Control by Stuart Russell.
This book explains the threats posed by AI and gives many non-intuitive examples of how we could lose control. The author is a Computer Science professor at UC Berkeley and authored the leading textbook on AI.
