MAILTO: brian@skeptoid.com; Please include as much supporting information as you have. Links to YouTube videos will not be watched, so write up a summary instead.
SUBJECT: Podcast Episode Request - AI Extinction Risk

Dear Brian,

I am impressed at how you take on kooky-sounding topics and give them a fair hearing. There’s a topic I’ve been looking into for the past year, and would love your sceptical perspective on: AI extinction risk.

The claim is that aligning artificial intelligence to human goals is extremely difficult and needs to be prioritised over making AIs more capable. I find the claim logical, and supported by recent issues found in AIs. I am interested how you interpret the research and what conclusion you come to.

Now is a particularly important time to know the truth about this. The majority of AI researchers, superforecasters, and one of the most popular models for AI development agree "transformative AI is most likely in the period between about 10 and 40 years from now" (https://asteriskmag.com/issues/03/through-a-glass-darkly). This is concerning, because most notable AI scientists and notable figures signed the statement "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war." (https://www.safe.ai/statement-on-ai-risk). 

I believe that your considered view would help people be informed, so the AI regulations many countries are proposing will include the will of the people. 

Kind regards,
Peter
